import time
import asyncio
import json
import sys
import os
import wave
from collections import deque
from threading import Lock
from io import BytesIO

# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ð»Ð¾Ð³Ð³ÐµÑ€
from Logger import logger

#os.environ['TORCH_FORCE_WEIGHTS_ONLY_LOAD'] = '1'

class AudioState:
    """ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð°ÑƒÐ´Ð¸Ð¾, Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð²Ð½ÐµÑˆÐ½Ð¸Ñ… Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐº."""
    def __init__(self):
        self.is_recording = False
        self.audio_buffer = []
        self.last_sound_time = time.time()
        self.is_playing = False
        self.lock = asyncio.Lock()
        self.vc = None
        self.max_buffer_size = 9999999

    async def add_to_buffer(self, data):
        async with self.lock:
            if len(self.audio_buffer) >= self.max_buffer_size:
                self.audio_buffer = self.audio_buffer[-self.max_buffer_size // 2:]
            self.audio_buffer.append(data.copy())

audio_state = AudioState()


class SpeechRecognition:
    # --- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ---
    microphone_index = 0
    active = True
    _recognizer_type = "vosk"  # 'google', 'vosk' Ð¸Ð»Ð¸ 'gigaam'
    vosk_model = "vosk-model-small-ru-0.22"
    gigaam_model = "v2_rnnt"  # ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ GigaAM

    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ VAD-Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² (Vosk, GigaAM)
    VOSK_SAMPLE_RATE = 16000
    CHUNK_SIZE = 512
    VAD_THRESHOLD = 0.5
    VAD_SILENCE_TIMEOUT_SEC = 1.0
    VAD_POST_SPEECH_DELAY_SEC = 0.2
    VAD_PRE_BUFFER_DURATION_SEC = 0.3

    FAILED_AUDIO_DIR = "FailedAudios"

    # --- Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¸ Ð±ÑƒÑ„ÐµÑ€Ñ‹ ---
    _text_lock = Lock()
    _text_buffer = deque(maxlen=15)
    _current_text = ""
    _is_processing_audio = asyncio.Lock()
    _is_running = False

    # --- ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð»ÐµÐ½Ð¸Ð²Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐº Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ ---
    _torch = None
    _sd = None
    _np = None
    _sr = None
    _vosk_Model = None
    _vosk_KaldiRecognizer = None
    _vosk_SetLogLevel = None
    _silero_vad_loader = None
    _omegaconf = None
    _gigaam = None  # Ð”Ð»Ñ Ð¼Ð¾Ð´ÑƒÐ»Ñ gigaam

    # --- ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² ---
    _vosk_model_instance = None
    _vosk_rec_instance = None
    _silero_vad_model = None
    _gigaam_model_instance = None  # Ð”Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ gigaam

    @staticmethod
    def _init_dependencies():
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ JIT-Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð²ÑÐµÑ… Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐº."""
        if SpeechRecognition._recognizer_type == 'vosk':
            try:
                if SpeechRecognition._torch is None:
                    import torch
                    SpeechRecognition._torch = torch
                if SpeechRecognition._sd is None:
                    import sounddevice as sd
                    SpeechRecognition._sd = sd
                if SpeechRecognition._np is None:
                    import numpy as np
                    SpeechRecognition._np = np
                
                if SpeechRecognition._vosk_Model is None:
                    from vosk import Model, KaldiRecognizer, SetLogLevel
                    SpeechRecognition._vosk_Model = Model
                    SpeechRecognition._vosk_KaldiRecognizer = KaldiRecognizer
                    SpeechRecognition._vosk_SetLogLevel = SetLogLevel
                    SpeechRecognition._vosk_SetLogLevel(-1)
                
                if SpeechRecognition._silero_vad_loader is None:
                    from silero_vad import load_silero_vad
                    SpeechRecognition._silero_vad_loader = load_silero_vad
                return True
            except ImportError as e:
                logger.critical(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ: {e}. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ 'torch', 'sounddevice', 'numpy', 'vosk' Ð¸ 'silero-vad'.")
                return False

        elif SpeechRecognition._recognizer_type == 'gigaam':
            try:
                # ÐžÐ±Ñ‰Ð¸Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ VAD
                if SpeechRecognition._torch is None:
                    import torch
                    import omegaconf
                    import typing
                    import collections
                    torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])
                    torch.serialization.add_safe_globals([omegaconf.base.ContainerMetadata])
                    torch.serialization.add_safe_globals([typing.Any])
                    torch.serialization.add_safe_globals([dict])
                    torch.serialization.add_safe_globals([collections.defaultdict])
                    torch.serialization.add_safe_globals([omegaconf.nodes.AnyNode])
                    torch.serialization.add_safe_globals([omegaconf.nodes.Metadata])
                    torch.serialization.add_safe_globals([omegaconf.listconfig.ListConfig])
                    torch.serialization.add_safe_globals([list])
                    torch.serialization.add_safe_globals([int])
                    # torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])
                    # torch.serialization.safe_globals([omegaconf.base.ContainerMetadata])
                    # torch.serialization.safe_globals([[typing.Any]])
                    logger.warning("TORCH ADDED SAFE GLOBALS!")
                    SpeechRecognition._torch = torch
                if SpeechRecognition._sd is None:
                    import sounddevice as sd
                    SpeechRecognition._sd = sd
                if SpeechRecognition._np is None:
                    import numpy as np
                    SpeechRecognition._np = np
                if SpeechRecognition._silero_vad_loader is None:
                    from silero_vad import load_silero_vad
                    SpeechRecognition._silero_vad_loader = load_silero_vad
                
                # Ð—Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÑŒ GigaAM
                if SpeechRecognition._gigaam is None:
                    import gigaam
                    SpeechRecognition._gigaam = gigaam
                return True
            except ImportError as e:
                logger.critical(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ: {e}. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ 'torch', 'sounddevice', 'numpy', 'silero-vad' Ð¸ 'gigaam'.")
                return False

        elif SpeechRecognition._recognizer_type == 'google':
            try:
                if SpeechRecognition._sr is None:
                    import speech_recognition as sr
                    SpeechRecognition._sr = sr
                return True
            except ImportError as e:
                logger.critical(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ 'speech_recognition': {e}.")
                return False
        return False

    @staticmethod
    def set_recognizer_type(recognizer_type: str):
        if recognizer_type in ["google", "vosk", "gigaam"]:
            SpeechRecognition._recognizer_type = recognizer_type
            logger.info(f"Ð¢Ð¸Ð¿ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ Ð½Ð°: {recognizer_type}")
        else:
            logger.warning(f"ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»Ñ: {recognizer_type}. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ 'google'.")

    @staticmethod
    def receive_text() -> str:
        with SpeechRecognition._text_lock:
            result = " ".join(SpeechRecognition._text_buffer).strip()
            SpeechRecognition._text_buffer.clear()
            SpeechRecognition._current_text = ""
            return result

    @staticmethod
    def list_microphones():
        if SpeechRecognition._sd is None:
            try:
                import sounddevice as sd
                SpeechRecognition._sd = sd
            except ImportError:
                logger.error("Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° 'sounddevice' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð´Ð»Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð° ÑÐ¿Ð¸ÑÐºÐ° Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð¾Ð².")
                return ["ÐžÑˆÐ¸Ð±ÐºÐ°: Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° sounddevice Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°"]
        
        try:
            devices = SpeechRecognition._sd.query_devices()
            input_devices = [dev['name'] for dev in devices if dev['max_input_channels'] > 0]
            return input_devices if input_devices else ["ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹"]
        except Exception as e:
            logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð¾Ð²: {e}")
            return [f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}"]

    @staticmethod
    async def handle_voice_message(recognized_text: str) -> None:
        text_clean = recognized_text.strip()
        if text_clean:
            with SpeechRecognition._text_lock:
                SpeechRecognition._text_buffer.append(text_clean)
                SpeechRecognition._current_text += f"{text_clean}. "

    @staticmethod
    def _init_vosk_recognizer():
        if SpeechRecognition._vosk_model_instance is None:
            model_path = f"SpeechRecognitionModels/Vosk/{SpeechRecognition.vosk_model}"
            try:
                SpeechRecognition._vosk_model_instance = SpeechRecognition._vosk_Model(model_path)
                logger.info(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Vosk Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¸Ð·: {model_path}")
            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Vosk Ð¸Ð· {model_path}: {e}")
                return False
        if SpeechRecognition._vosk_rec_instance is None:
            SpeechRecognition._vosk_rec_instance = SpeechRecognition._vosk_KaldiRecognizer(
                SpeechRecognition._vosk_model_instance, SpeechRecognition.VOSK_SAMPLE_RATE
            )
            logger.info(f"Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒ Vosk Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ñ sample_rate={SpeechRecognition.VOSK_SAMPLE_RATE}.")
        return True

    @staticmethod
    def _init_gigaam_recognizer():
        if SpeechRecognition._gigaam_model_instance is None:
            if SpeechRecognition._gigaam is None:
                logger.error("ÐœÐ¾Ð´ÑƒÐ»ÑŒ GigaAM Ð½Ðµ Ð±Ñ‹Ð» Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
                return False
            try:
                logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ GigaAM: {SpeechRecognition.gigaam_model}...")
                model = SpeechRecognition._gigaam.load_model(SpeechRecognition.gigaam_model)
                SpeechRecognition._gigaam_model_instance = model
                logger.info(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ GigaAM '{SpeechRecognition.gigaam_model}' ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°.")
            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ GigaAM '{SpeechRecognition.gigaam_model}': {e}")
                return False
        return True

    @staticmethod
    def _init_silero_vad():
        if SpeechRecognition._silero_vad_model is None:
            if SpeechRecognition._silero_vad_loader is None:
                logger.error("Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Silero VAD Ð½Ðµ Ð±Ñ‹Ð»Ð° Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð°.")
                return False
            try:
                model = SpeechRecognition._silero_vad_loader()
                SpeechRecognition._silero_vad_model = model
                logger.info("ÐœÐ¾Ð´ÐµÐ»ÑŒ Silero VAD ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ñ‡ÐµÑ€ÐµÐ· pip-Ð¿Ð°ÐºÐµÑ‚.")
            except Exception as e:
                logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Silero VAD. ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
                return False
        return True

    @staticmethod
    async def _recognize_vosk_from_buffer(audio_data: "np.ndarray") -> None:
        np = SpeechRecognition._np
        rec = SpeechRecognition._vosk_rec_instance
        if rec is None or np is None:
            logger.error("Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒ Vosk Ð¸Ð»Ð¸ Numpy Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
            return

        audio_data_int16 = (audio_data * 32767).astype(np.int16)
        
        rec.AcceptWaveform(audio_data_int16.tobytes())
        result_json = json.loads(rec.FinalResult())
        rec.Reset()

        if 'text' in result_json and result_json['text']:
            recognized_text = result_json['text']
            logger.info(f"Vosk Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: {recognized_text}")
            await SpeechRecognition.handle_voice_message(recognized_text)
        else:
            logger.info("Vosk Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð» Ñ‚ÐµÐºÑÑ‚. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð°...")
            try:
                os.makedirs(SpeechRecognition.FAILED_AUDIO_DIR, exist_ok=True)
                timestamp = int(time.time())
                filename = os.path.join(SpeechRecognition.FAILED_AUDIO_DIR, f"failed_{timestamp}.wav")
                with wave.open(filename, 'wb') as wf:
                    wf.setnchannels(1)
                    wf.setsampwidth(2)
                    wf.setframerate(SpeechRecognition.VOSK_SAMPLE_RATE)
                    wf.writeframes(audio_data_int16.tobytes())
                logger.info(f"Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð²: {filename}")
            except Exception as e:
                logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚: {e}")

    @staticmethod
    async def _recognize_gigaam_from_buffer(audio_data: "np.ndarray") -> None:
        model = SpeechRecognition._gigaam_model_instance
        if model is None:
            logger.error("Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒ GigaAM Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
            return

        np = SpeechRecognition._np
        TEMP_AUDIO_DIR = "TempAudios"
        os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)
        temp_filepath = os.path.join(TEMP_AUDIO_DIR, f"temp_gigaam_{time.time_ns()}.wav")
        
        recognized_successfully = False
        try:
            audio_data_int16 = (audio_data * 32767).astype(np.int16)
            with wave.open(temp_filepath, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(SpeechRecognition.VOSK_SAMPLE_RATE)
                wf.writeframes(audio_data_int16.tobytes())

            transcription = model.transcribe(temp_filepath)
            if transcription and transcription.strip() != '':
                recognized_text = transcription  
                logger.info(f"GigaAM Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: {recognized_text}")
                await SpeechRecognition.handle_voice_message(recognized_text)
                recognized_successfully = True
            else:
                logger.info("GigaAM Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð» Ñ‚ÐµÐºÑÑ‚.")

        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ GigaAM: {e}")

        finally:
            if os.path.exists(temp_filepath):
                try:
                    os.remove(temp_filepath)
                except OSError as e:
                    logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» {temp_filepath}: {e}")

        if not recognized_successfully:
            logger.info("Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð° Ð² Ð¿Ð°Ð¿ÐºÑƒ Failed...")
            try:
                os.makedirs(SpeechRecognition.FAILED_AUDIO_DIR, exist_ok=True)
                timestamp = int(time.time())
                filename = os.path.join(SpeechRecognition.FAILED_AUDIO_DIR, f"failed_{timestamp}.wav")
                
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑƒÐ¶Ðµ ÑÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ ÐµÑÑ‚ÑŒ, Ð¸Ð»Ð¸ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð½Ð¾Ð²Ð¾
                if 'audio_data_int16' not in locals():
                    audio_data_int16 = (audio_data * 32767).astype(np.int16)

                with wave.open(filename, 'wb') as wf:
                    wf.setnchannels(1)
                    wf.setsampwidth(2)
                    wf.setframerate(SpeechRecognition.VOSK_SAMPLE_RATE)
                    wf.writeframes(audio_data_int16.tobytes())
                logger.info(f"Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð²: {filename}")
            except Exception as e:
                logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚: {e}")

    @staticmethod
    async def _process_audio_task(audio_data: "np.ndarray"):
        async with SpeechRecognition._is_processing_audio:
            if SpeechRecognition._recognizer_type == "vosk":
                await SpeechRecognition._recognize_vosk_from_buffer(audio_data)
            elif SpeechRecognition._recognizer_type == "gigaam":
                await SpeechRecognition._recognize_gigaam_from_buffer(audio_data)

    @staticmethod
    async def live_recognition() -> None:
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´, Ð·Ð°Ð¿ÑƒÑÐºÐ°ÑŽÑ‰Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹."""
        try:
            if not SpeechRecognition._init_dependencies():
                return

            if SpeechRecognition._recognizer_type == "vosk":
                if not SpeechRecognition._init_silero_vad() or not SpeechRecognition._init_vosk_recognizer():
                    logger.error("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Vosk Ð¸Ð»Ð¸ Silero VAD. Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾.")
                    return

                sd = SpeechRecognition._sd
                np = SpeechRecognition._np
                torch = SpeechRecognition._torch
                vad_model = SpeechRecognition._silero_vad_model
                
                silence_chunks_needed = int(SpeechRecognition.VAD_SILENCE_TIMEOUT_SEC * SpeechRecognition.VOSK_SAMPLE_RATE / SpeechRecognition.CHUNK_SIZE)
                pre_buffer_size = int(SpeechRecognition.VAD_PRE_BUFFER_DURATION_SEC * SpeechRecognition.VOSK_SAMPLE_RATE / SpeechRecognition.CHUNK_SIZE)
                
                try:
                    mic_name = SpeechRecognition.list_microphones()[SpeechRecognition.microphone_index]
                    logger.info(f"Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½: {mic_name}")
                except IndexError:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ°: Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð¼ {SpeechRecognition.microphone_index} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½.")
                    return

                logger.info("ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ (Vosk + Silero VAD Ñ Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹)...")

                pre_speech_buffer = deque(maxlen=pre_buffer_size)
                speech_buffer = []
                is_speaking = False
                silence_counter = 0

                with sd.InputStream(
                    samplerate=SpeechRecognition.VOSK_SAMPLE_RATE,
                    channels=1,
                    dtype='float32',
                    blocksize=SpeechRecognition.CHUNK_SIZE,
                    device=SpeechRecognition.microphone_index
                ) as stream:
                    while SpeechRecognition.active:
                        audio_chunk, overflowed = stream.read(SpeechRecognition.CHUNK_SIZE)
                        if overflowed:
                            logger.warning("ÐŸÐµÑ€ÐµÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð±ÑƒÑ„ÐµÑ€Ð° Ð°ÑƒÐ´Ð¸Ð¾Ð¿Ð¾Ñ‚Ð¾ÐºÐ°!")

                        if not is_speaking:
                            pre_speech_buffer.append(audio_chunk)

                        audio_tensor = torch.from_numpy(audio_chunk.flatten())
                        speech_prob = vad_model(audio_tensor, SpeechRecognition.VOSK_SAMPLE_RATE).item()

                        if speech_prob > SpeechRecognition.VAD_THRESHOLD:
                            if not is_speaking:
                                logger.debug("ðŸŸ¢ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ€ÐµÑ‡Ð¸. Ð—Ð°Ñ…Ð²Ð°Ñ‚ Ð¸Ð· Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð°.")
                                is_speaking = True
                                speech_buffer.clear()
                                speech_buffer.extend(list(pre_speech_buffer))
                            
                            speech_buffer.append(audio_chunk)
                            silence_counter = 0
                        
                        elif is_speaking:
                            speech_buffer.append(audio_chunk)
                            silence_counter += 1
                            if silence_counter > silence_chunks_needed:
                                logger.debug("ðŸ”´ ÐšÐ¾Ð½ÐµÑ† Ñ€ÐµÑ‡Ð¸. ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð½Ð° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ.")
                                audio_to_process = np.concatenate(speech_buffer)
                                
                                is_speaking = False
                                speech_buffer.clear()
                                silence_counter = 0
                                
                                asyncio.create_task(SpeechRecognition._process_audio_task(audio_to_process))
                        
                        await asyncio.sleep(0.01)
            
            elif SpeechRecognition._recognizer_type == "gigaam":
                if not SpeechRecognition._init_silero_vad() or not SpeechRecognition._init_gigaam_recognizer():
                    logger.error("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ GigaAM Ð¸Ð»Ð¸ Silero VAD. Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾.")
                    return

                sd = SpeechRecognition._sd
                np = SpeechRecognition._np
                torch = SpeechRecognition._torch
                vad_model = SpeechRecognition._silero_vad_model
                
                silence_chunks_needed = int(SpeechRecognition.VAD_SILENCE_TIMEOUT_SEC * SpeechRecognition.VOSK_SAMPLE_RATE / SpeechRecognition.CHUNK_SIZE)
                pre_buffer_size = int(SpeechRecognition.VAD_PRE_BUFFER_DURATION_SEC * SpeechRecognition.VOSK_SAMPLE_RATE / SpeechRecognition.CHUNK_SIZE)
                
                try:
                    mic_name = SpeechRecognition.list_microphones()[SpeechRecognition.microphone_index]
                    logger.info(f"Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½: {mic_name}")
                except IndexError:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ°: Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð¼ {SpeechRecognition.microphone_index} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½.")
                    return

                logger.info("ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ (GigaAM + Silero VAD Ñ Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹)...")

                pre_speech_buffer = deque(maxlen=pre_buffer_size)
                speech_buffer = []
                is_speaking = False
                silence_counter = 0

                with sd.InputStream(
                    samplerate=SpeechRecognition.VOSK_SAMPLE_RATE,
                    channels=1,
                    dtype='float32',
                    blocksize=SpeechRecognition.CHUNK_SIZE,
                    device=SpeechRecognition.microphone_index
                ) as stream:
                    while SpeechRecognition.active:
                        audio_chunk, overflowed = stream.read(SpeechRecognition.CHUNK_SIZE)
                        if overflowed:
                            logger.warning("ÐŸÐµÑ€ÐµÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð±ÑƒÑ„ÐµÑ€Ð° Ð°ÑƒÐ´Ð¸Ð¾Ð¿Ð¾Ñ‚Ð¾ÐºÐ°!")

                        if not is_speaking:
                            pre_speech_buffer.append(audio_chunk)

                        audio_tensor = torch.from_numpy(audio_chunk.flatten())
                        speech_prob = vad_model(audio_tensor, SpeechRecognition.VOSK_SAMPLE_RATE).item()

                        if speech_prob > SpeechRecognition.VAD_THRESHOLD:
                            if not is_speaking:
                                logger.debug("ðŸŸ¢ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ€ÐµÑ‡Ð¸. Ð—Ð°Ñ…Ð²Ð°Ñ‚ Ð¸Ð· Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð°.")
                                is_speaking = True
                                speech_buffer.clear()
                                speech_buffer.extend(list(pre_speech_buffer))
                            
                            speech_buffer.append(audio_chunk)
                            silence_counter = 0
                        
                        elif is_speaking:
                            speech_buffer.append(audio_chunk)
                            silence_counter += 1
                            if silence_counter > silence_chunks_needed:
                                logger.debug("ðŸ”´ ÐšÐ¾Ð½ÐµÑ† Ñ€ÐµÑ‡Ð¸. ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð½Ð° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ.")
                                audio_to_process = np.concatenate(speech_buffer)
                                
                                is_speaking = False
                                speech_buffer.clear()
                                silence_counter = 0
                                
                                asyncio.create_task(SpeechRecognition._process_audio_task(audio_to_process))
                        
                        await asyncio.sleep(0.01)

            elif SpeechRecognition._recognizer_type == "google":
                sr = SpeechRecognition._sr
                recognizer = sr.Recognizer()
                google_sample_rate = 44100
                with sr.Microphone(device_index=SpeechRecognition.microphone_index, sample_rate=google_sample_rate,
                                   chunk_size=SpeechRecognition.CHUNK_SIZE) as source:
                    logger.info(f"Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½: {sr.Microphone.list_microphone_names()[SpeechRecognition.microphone_index]}")
                    recognizer.adjust_for_ambient_noise(source)
                    logger.info("Ð¡ÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ñ‡Ñ‚Ð¾-Ð½Ð¸Ð±ÑƒÐ´ÑŒ (Google)...")
                    while SpeechRecognition.active:
                        try:
                            audio = await asyncio.get_event_loop().run_in_executor(None, lambda: recognizer.listen(source, timeout=5))
                            text = await asyncio.get_event_loop().run_in_executor(None, lambda: recognizer.recognize_google(audio, language="ru-RU"))
                            if text: await SpeechRecognition.handle_voice_message(text)
                        except sr.WaitTimeoutError: pass
                        except sr.UnknownValueError: pass
                        except Exception as e:
                            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ð¸ Google: {e}")
                            break
        except Exception as e:
            logger.error(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² Ñ†Ð¸ÐºÐ»Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ: {e}", exc_info=True)
        finally:
            SpeechRecognition._is_running = False
            logger.info("Ð¦Ð¸ÐºÐ» Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½.")


    @staticmethod
    async def speach_recognition_start_async():
        await SpeechRecognition.live_recognition()

    @staticmethod
    def speach_recognition_start(device_id: int, loop):
        if SpeechRecognition._is_running:
            logger.warning("ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ, ÐºÐ¾Ð³Ð´Ð° Ð¾Ð½Ð¾ ÑƒÐ¶Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¾. Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÑ‚ÑÑ.")
            return

        SpeechRecognition._is_running = True
        SpeechRecognition.active = True
        SpeechRecognition.microphone_index = device_id
        asyncio.run_coroutine_threadsafe(SpeechRecognition.speach_recognition_start_async(), loop)

    @staticmethod
    async def get_current_text() -> str:
        with SpeechRecognition._text_lock:
            return SpeechRecognition._current_text.strip()