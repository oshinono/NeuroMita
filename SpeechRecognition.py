import time
from io import BytesIO
import asyncio
import logging
import soundfile as sf
import numpy as np
import speech_recognition as sr
import sounddevice as sd
from collections import deque
from threading import Lock
from Logger import logger
import httpx
import json
import wave
import sys
from vosk import Model, KaldiRecognizer, SetLogLevel
import io

# You can set log level to -1 to disable debug messages
SetLogLevel(1) # –í–æ–∑–≤—Ä–∞—â–µ–Ω–æ –∫ 0

class AudioState:
    def __init__(self):
        self.is_recording = False
        self.audio_buffer = []
        self.last_sound_time = time.time()
        self.is_playing = False
        self.lock = asyncio.Lock()
        self.vc = None
        self.max_buffer_size = 9999999

    async def add_to_buffer(self, data):
        async with self.lock:
            if len(self.audio_buffer) >= self.max_buffer_size:
                self.audio_buffer = self.audio_buffer[-self.max_buffer_size // 2:]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50%
            self.audio_buffer.append(data.copy())


audio_state = AudioState()


class SpeechRecognition:
    user_input = ""
    microphone_index = 0
    active = True
    _recognizer_type = "google"  # 'google' –∏–ª–∏ 'vosk'
   # vosk_model = "vosk-model-ru-0.10" #vosk-model-small-ru
    vosk_model = "vosk-model-small-ru-0.22"

    SAMPLE_RATE = 32000
    CHUNK_SIZE = 1024 # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
    TIMEOUT_MESSAGE = True
    SILENCE_THRESHOLD = 0.02  # –ü–æ—Ä–æ–≥ —Ç–∏—à–∏–Ω—ã
    SILENCE_DURATION = 4  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–∏—à–∏–Ω—ã –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏
    MAX_RECORDING_DURATION = 15 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ (—Å–µ–∫)
    SILENCE_TIMEOUT = 2.0
    MIN_RECORDING = 1.0
    MIN_RECORDING_DURATION = 1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏
    BUFFER_TIMEOUT = 0.05
    VOSK_PROCESS_INTERVAL = 0.3 # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
    _text_lock = Lock()
    _text_buffer = deque(maxlen=15)  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ñ—Ä–∞–∑
    _current_text = ""
    _last_delimiter = ". "

    @staticmethod
    def set_recognizer_type(recognizer_type: str):
        if recognizer_type in ["google", "vosk"]:
            SpeechRecognition._recognizer_type = recognizer_type
            logger.info(f"–¢–∏–ø —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞: {recognizer_type}")
        else:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è: {recognizer_type}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'google'.")


    @staticmethod
    def receive_text() -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ —Å–±—Ä–æ—Å —Ç–µ–∫—Å—Ç–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥)"""
        with SpeechRecognition._text_lock:
            result = " ".join(SpeechRecognition._text_buffer).strip()
            SpeechRecognition._text_buffer.clear()
            SpeechRecognition._current_text = ""
            #logger.debug(f"Returned text: {result}")
            return result

    @staticmethod
    def list_microphones():
        return sr.Microphone.list_microphone_names()

    @staticmethod
    async def handle_voice_message(recognized_text: str) -> None:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        text_clean = recognized_text.strip()
        if text_clean:
            with SpeechRecognition._text_lock:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
                last_char = SpeechRecognition._current_text[-1] if SpeechRecognition._current_text else ""
                delimiter = "" if last_char in {'.', '!', '?', ','} else " "

                SpeechRecognition._text_buffer.append(text_clean)
                SpeechRecognition._current_text += f"{delimiter}{text_clean}"

    @staticmethod
    def _stereo_to_mono(audio_data):
        return np.mean(audio_data, axis=1, dtype=audio_data.dtype)

    _vosk_model_instance = None
    _vosk_rec_instance = None

    @staticmethod
    def _init_vosk_recognizer():
        if SpeechRecognition._vosk_model_instance is None:
            model_path = f"SpeechRecognitionModels/Vosk/{SpeechRecognition.vosk_model}"
            try:
                SpeechRecognition._vosk_model_instance = Model(model_path)
                logger.info(f"–ú–æ–¥–µ–ª—å Vosk –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {model_path}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ Vosk –∏–∑ {model_path}: {e}")
                return False
        
        if SpeechRecognition._vosk_rec_instance is None:
            SpeechRecognition._vosk_rec_instance = KaldiRecognizer(SpeechRecognition._vosk_model_instance, SpeechRecognition.SAMPLE_RATE)
            SpeechRecognition._vosk_rec_instance.SetWords(True)
            SpeechRecognition._vosk_rec_instance.SetPartialWords(True)
            logger.info("–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å Vosk –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return True

    @staticmethod
    async def recognize_vosk(audio_data: np.ndarray) -> str | None:
        if not SpeechRecognition._init_vosk_recognizer():
            return None

        # Vosk –æ–∂–∏–¥–∞–µ—Ç int16, –∞ sounddevice –¥–∞–µ—Ç float32.
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º float32 –≤ int16
        audio_data_int16 = (audio_data * 32767).astype(np.int16)

        # –°–æ–∑–¥–∞–µ–º in-memory wave file
        bytes_io = io.BytesIO()
        with wave.open(bytes_io, 'wb') as mono_wf:
            mono_wf.setnchannels(1)
            mono_wf.setsampwidth(2)  # 2 bytes for int16
            mono_wf.setframerate(SpeechRecognition.SAMPLE_RATE)
            mono_wf.writeframes(audio_data_int16.tobytes())
        
        bytes_io.seek(0)
        
        recognized_text = ''
        rec = SpeechRecognition._vosk_rec_instance # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –∞—É–¥–∏–æ
        rec.Reset()

        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ in-memory wave file
        with wave.open(bytes_io, 'rb') as wf:
            while True:
                data = wf.readframes(4000) # –ß–∏—Ç–∞–µ–º –ø–æ 4000 —Ñ—Ä–µ–π–º–æ–≤
                if len(data) == 0:
                    break
                if rec.AcceptWaveform(data):
                    stepResult = rec.Result()
                    recognized_json = json.loads(stepResult)
                    if 'text' in recognized_json and recognized_json['text']:
                        recognized_text += ' ' + recognized_json['text'] + '.'
                else:
                    # Partial results are not used for final text, but can be logged for debugging
                    pass
        
        final_result = rec.FinalResult()
        final_json = json.loads(final_result)
        if 'text' in final_json and final_json['text']:
            recognized_text += ' ' + final_json['text'] + '.'
        
        return recognized_text.strip()


    @staticmethod
    async def live_recognition() -> None:
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ-—Ä–∞–∑–Ω–æ–º—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è.
        # –î–ª—è Google –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è speech_recognition.Microphone.
        # –î–ª—è Vosk –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è sounddevice –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Vosk API.

        if SpeechRecognition._recognizer_type == "google":
            recognizer = sr.Recognizer()
            with sr.Microphone(device_index=SpeechRecognition.microphone_index, sample_rate=SpeechRecognition.SAMPLE_RATE,
                               chunk_size=SpeechRecognition.CHUNK_SIZE) as source:
                logger.info(
                    f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–∫—Ä–æ—Ñ–æ–Ω: {sr.Microphone.list_microphone_names()[SpeechRecognition.microphone_index]}")
                recognizer.adjust_for_ambient_noise(source)
                logger.info("–°–∫–∞–∂–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å (Google)...")

                while SpeechRecognition.active:
                    try:
                        audio = await asyncio.get_event_loop().run_in_executor(
                            None,
                            lambda: recognizer.listen(source, timeout=5)
                        )

                        text = await asyncio.get_event_loop().run_in_executor(
                            None,
                            lambda: recognizer.recognize_google(audio, language="ru-RU")
                        )
                        if not text:
                            text = await asyncio.get_event_loop().run_in_executor(
                                None,
                                lambda: recognizer.recognize_google(audio, language="en-EN")
                            )

                        if text:
                            await SpeechRecognition.handle_voice_message(text)

                    except sr.WaitTimeoutError:
                        if SpeechRecognition.TIMEOUT_MESSAGE:
                            ...
                    except sr.UnknownValueError:
                        ...
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ Google: {e}")
                        break
        elif SpeechRecognition._recognizer_type == "vosk":
            if not SpeechRecognition._init_vosk_recognizer():
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Vosk —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å. –û—Ç–º–µ–Ω–∞ live_recognition.")
                return

            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–∫—Ä–æ—Ñ–æ–Ω: {sr.Microphone.list_microphone_names()[SpeechRecognition.microphone_index]}")
            logger.info("–°–∫–∞–∂–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å (Vosk)...")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º sounddevice –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            with sd.InputStream(
                samplerate=SpeechRecognition.SAMPLE_RATE,
                channels=1,
                dtype='float32',
                blocksize=SpeechRecognition.CHUNK_SIZE,
                device=SpeechRecognition.microphone_index
            ) as stream:
                while SpeechRecognition.active:
                    try:
                        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ—Ç–æ–∫–∞
                        data, overflowed = stream.read(SpeechRecognition.CHUNK_SIZE)
                        if overflowed:
                            logger.warning("–ü–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–∞ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞!")

                        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö
                        if data.size > 0:
                            rms_val = np.sqrt(np.mean(data ** 2))
                            logger.debug(f"–ü–æ–ª—É—á–µ–Ω—ã –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã–µ. –†–∞–∑–º–µ—Ä: {data.size}, RMS: {rms_val:.4f}")
                            if rms_val < SpeechRecognition.SILENCE_THRESHOLD:
                                logger.debug("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ç–∏—à–∏–Ω–∞.")
                        else:
                            logger.debug("–ü–æ–ª—É—á–µ–Ω—ã –ø—É—Å—Ç—ã–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã–µ.")

                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º float32 –≤ int16 –¥–ª—è Vosk
                        audio_data_int16 = (data * 32767).astype(np.int16)
                        
                        # –ü–µ—Ä–µ–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Vosk
                        if SpeechRecognition._vosk_rec_instance.AcceptWaveform(audio_data_int16.tobytes()):
                            result_json = json.loads(SpeechRecognition._vosk_rec_instance.Result())
                            if 'text' in result_json and result_json['text']:
                                # –î–æ–±–∞–≤–ª—è–µ–º ucfirst –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å –ø—Ä–∏–º–µ—Ä–æ–º
                                recognized_text = result_json['text']
                                if recognized_text:
                                    recognized_text = recognized_text[:1].upper() + recognized_text[1:]
                                await SpeechRecognition.handle_voice_message(recognized_text)
                                logger.info(f"Vosk —Ä–∞—Å–ø–æ–∑–Ω–∞–ª: {recognized_text}")
                        else:
                            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                            partial_result_json = json.loads(SpeechRecognition._vosk_rec_instance.PartialResult())
                            if 'partial' in partial_result_json and partial_result_json['partial']:
                                # –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                                logger.debug(f"Vosk —á–∞—Å—Ç–∏—á–Ω—ã–π: {partial_result_json['partial']}")
                                pass
                        
                        await asyncio.sleep(SpeechRecognition.VOSK_PROCESS_INTERVAL) # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ Vosk –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏: {e}")
                        break
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏
            final_result = SpeechRecognition._vosk_rec_instance.FinalResult()
            final_json = json.loads(final_result)
            if 'text' in final_json and final_json['text']:
                recognized_text = final_json['text']
                if recognized_text:
                    recognized_text = recognized_text[:1].upper() + recognized_text[1:]
                await SpeechRecognition.handle_voice_message(recognized_text)
                logger.info(f"Vosk –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π: {recognized_text}")


    @staticmethod
    async def async_audio_callback(indata):
        try:
            current_time = time.time()
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º indata –≤ numpy array
            audio_data = np.frombuffer(indata, dtype=np.float32)
            rms = np.sqrt(np.mean(audio_data ** 2))

            async with audio_state.lock:
                if rms > SpeechRecognition.SILENCE_THRESHOLD:
                    audio_state.last_sound_time = current_time
                    if not audio_state.is_recording:
                        logger.debug("üü¢ –ù–∞—á–∞–ª–æ –∑–∞–ø–∏—Å–∏")
                        audio_state.is_recording = True
                    await audio_state.add_to_buffer(audio_data)

                elif audio_state.is_recording:
                    silence_duration = 4
                    audio_state.is_recording = False
                    await SpeechRecognition.process_audio()
                else:
                    logger.debug("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è –∑–∞–ø–∏—Å—å, —Å–±—Ä–æ—Å")
                    audio_state.audio_buffer.clear()
                    audio_state.is_recording = False

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –∫–æ–ª–±—ç–∫–µ: {str(e)}")

    @staticmethod
    async def process_audio():
        try:
            async with audio_state.lock:
                if not audio_state.audio_buffer:
                    return

                audio_data = np.concatenate(audio_state.audio_buffer)
                audio_state.audio_buffer.clear()

                text = None
                if SpeechRecognition._recognizer_type == "google":
                    with BytesIO() as buffer:
                        sf.write(buffer, audio_data, SpeechRecognition.SAMPLE_RATE, format='WAV')
                        buffer.seek(0)

                        try:
                            recognizer = sr.Recognizer()
                            with sr.AudioFile(buffer) as source:
                                audio = recognizer.record(source)
                                text = recognizer.recognize_google(audio, language="ru-RU")
                                logger.info(f"Google —Ä–∞—Å–ø–æ–∑–Ω–∞–ª: {text}")
                        except sr.UnknownValueError:
                            logger.warning("Google –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª —Ä–µ—á—å.")
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è Google: {str(e)}")
                elif SpeechRecognition._recognizer_type == "vosk":
                    text = await SpeechRecognition.recognize_vosk(audio_data)

                if text:
                    await SpeechRecognition.handle_voice_message(text)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    @staticmethod
    async def recognize_speech(audio_buffer):
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞,
        # –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è AudioFile-–ø–æ–¥–æ–±–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º.
        # –î–ª—è Vosk API –Ω–∞–º –Ω—É–∂–µ–Ω numpy array.
        # –ü–æ—ç—Ç–æ–º—É, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω Vosk, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å audio_buffer –≤ numpy array.
        # –ò–ª–∏ –∂–µ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è Google.
        # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–∏–º –µ–≥–æ –¥–ª—è Google, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç AudioFile.
        # –ï—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è Vosk –∑–¥–µ—Å—å, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å.
        recognizer = sr.Recognizer()
        text = None

        if SpeechRecognition._recognizer_type == "google":
            try:
                with sr.AudioFile(audio_buffer) as source:
                    audio = recognizer.record(source)

                text = recognizer.recognize_google(audio, language="ru-RU")
                if not text:
                    text = recognizer.recognize_google(audio, language="en-EN")
                return text
            except sr.UnknownValueError:
                logger.error("Google: –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å")
                return None
            except sr.RequestError as e:
                logger.error(f"Google: –û—à–∏–±–∫–∞ API: {e}")
                return None
        elif SpeechRecognition._recognizer_type == "vosk":
            # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å audio_buffer –≤ numpy array
            # –≠—Ç–æ —Å–ª–æ–∂–Ω–µ–µ, —Ç–∞–∫ –∫–∞–∫ audio_buffer –º–æ–∂–µ—Ç –±—ã—Ç—å BytesIO –∏–ª–∏ –¥—Ä—É–≥–∏–º –æ–±—ä–µ–∫—Ç–æ–º
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –ø–æ–∫–∞ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å Google
            logger.warning("recognize_speech –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Vosk –Ω–∞–ø—Ä—è–º—É—é —Å —Ç–µ–∫—É—â–∏–º —Ç–∏–ø–æ–º audio_buffer.")
            return None

    @staticmethod
    async def speach_recognition_start_async_other_system():
        while SpeechRecognition.active:
            try:
                await SpeechRecognition.async_audio_callback(0)
                await asyncio.sleep(0.1)  # –£–º–µ–Ω—å—à–∏–º –∏–Ω—Ç–µ—Ä–≤–∞–ª
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ speach_recognition_start_async_other_system: {e}")

    @staticmethod
    async def speach_recognition_start_async():
        await SpeechRecognition.live_recognition()

    @staticmethod
    def speach_recognition_start(device_id: int, loop):
        SpeechRecognition.microphone_index = device_id
        asyncio.run_coroutine_threadsafe(SpeechRecognition.speach_recognition_start_async(), loop)


    @staticmethod
    async def audio_monitoring():
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∞—É–¥–∏–æ–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            loop = asyncio.get_event_loop()
            with sd.InputStream(
                    callback=lambda indata, *_: asyncio.run_coroutine_threadsafe(SpeechRecognition.async_audio_callback(indata), loop),
                    channels=1,
                    samplerate=SpeechRecognition.SAMPLE_RATE,
                    blocksize=SpeechRecognition.CHUNK_SIZE,
                    device=SpeechRecognition.microphone_index
            ):
                while SpeechRecognition.active:
                    await asyncio.sleep(0.1)
        except Exception as e:
            logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

    @staticmethod
    async def get_current_text() -> str:
        async with SpeechRecognition._text_lock:
            return SpeechRecognition._current_text.strip()
